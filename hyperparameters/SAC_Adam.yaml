# Tuned SAC hyperparameters with Adam optimizer

# Reward: channel capacity
MobilityCFmMIMOEnv-ch_capacity-v0:
  policy: 'MlpPolicy'
  learning_rate: !!float 7.252237144216058e-05
  buffer_size: 100000
  batch_size: 1024
  ent_coef: 'auto'
  gamma: 0.9
  tau: 0.001
  train_freq: 32
  gradient_steps: 32
  learning_starts: 0
  policy_kwargs:
    log_std_init: 0.448800043823388
    net_arch: [64, 64]


# Reward: geo mean SE
MobilityCFmMIMOEnv-geo_mean_se-v0:
  n_timesteps: !!float 1e4
  policy: 'MlpPolicy'
  learning_rate: !!float 4.735493859730957e-05
  buffer_size: 100000
  batch_size: 2048
  ent_coef: 'auto'
  gamma: 0.9999
  tau: 0.001
  train_freq: 32
  gradient_steps: 32
  learning_starts: 10
  policy_kwargs:
    log_std_init: -0.0821970906267695
    net_arch: [64, 64]


# Reward: mean SE
MobilityCFmMIMOEnv-mean_se-v0:
  n_timesteps: !!float 1e4
  policy: 'MlpPolicy'
  learning_rate: !!float 5.696013431825893e-05
  buffer_size: 100000
  batch_size: 512
  ent_coef: 'auto'
  gamma: 0.95
  tau: 0.001
  train_freq: 64
  gradient_steps: 64
  learning_starts: 1000
  policy_kwargs:
    log_std_init: 0.33028825683887586
    net_arch: [64, 64]


# Reward: min SE
MobilityCFmMIMOEnv-min_se-v0:
  policy: 'MlpPolicy'
  learning_rate: !!float 3.8507890855295114e-05
  buffer_size: 1000000
  batch_size: 256
  ent_coef: 'auto'
  gamma: 0.98
  tau: 0.001
  train_freq: 1
  gradient_steps: 1
  learning_starts: 100
  policy_kwargs:
    log_std_init: -3.117646919381182,
    net_arch: [400, 300]


# Reward: sum SE
MobilityCFmMIMOEnv-sum_se-v0:
  policy: 'MlpPolicy'
  learning_rate: 0.04881669230937287
  buffer_size: 1000000
  batch_size: 1024
  ent_coef: 'auto'
  gamma: 0.9
  tau: 0.01
  train_freq: 256
  gradient_steps: 256
  learning_starts: 0
  policy_kwargs:
    log_std_init: -3.9280969748234518
    net_arch: [256, 256]