# Tuned SAC hyperparameters for MobilityCFmMIMOEnv-v0, MobilityCFmMIMOEnv-v1, MobilityCFmMIMOEnv-v2, MobilityCFmMIMOEnv-v3

# Reward: mean SE (max step 20)
MobilityCFmMIMOEnv-v0:
  policy: 'MlpPolicy'
  learning_rate: !!float 1.3169470844092746e-05
  buffer_size: 10000
  batch_size: 16
  ent_coef: 'auto'
  gamma: 0.99
  tau: 0.01
  train_freq: 4
  gradient_steps: 4
  learning_starts: 0
  policy_kwargs: "dict(log_std_init=0.2720120560193485, net_arch=[400, 300], use_sde=False)"


# Reward: mean SE (max step 10)
MobilityCFmMIMOEnv-v1:
  n_timesteps: !!float 1e4
  policy: 'MlpPolicy'
  learning_rate: !!float 5.696013431825893e-05
  buffer_size: 100000
  batch_size: 512
  ent_coef: 'auto'
  gamma: 0.95
  tau: 0.001
  train_freq: 64
  gradient_steps: 64
  learning_starts: 1000
  policy_kwargs: "dict(log_std_init=0.33028825683887586, net_arch=[64, 64])"


# Reward: min SE
MobilityCFmMIMOEnv-v2:
  n_timesteps: !!float 1e4
  policy: 'MlpPolicy'
  learning_rate: !!float 3.8507890855295114e-05
  buffer_size: 1000000
  batch_size: 256
  ent_coef: 'auto'
  gamma: 0.98
  tau: 0.001
  train_freq: 1
  gradient_steps: 1
  learning_starts: 100
  policy_kwargs: "dict(log_std_init=-3.117646919381182, net_arch=[400, 300])"


# Reward: sum SE
MobilityCFmMIMOEnv-v3:
  n_timesteps: !!float 1e4
  policy: 'MlpPolicy'
  learning_rate: 0.04881669230937287
  buffer_size: 1000000
  batch_size: 1024
  ent_coef: 'auto'
  gamma: 0.9
  tau: 0.01
  train_freq: 256
  gradient_steps: 256
  learning_starts: 0
  policy_kwargs: "dict(log_std_init=-3.9280969748234518, net_arch=[256, 256])"


# Reward: geometric mean SE
MobilityCFmMIMOEnv-v4:
  n_timesteps: !!float 1e4
  policy: 'MlpPolicy'
  learning_rate: !!float 4.735493859730957e-05
  buffer_size: 100000
  batch_size: 2048
  ent_coef: 'auto'
  gamma: 0.9999
  tau: 0.001
  train_freq: 32
  gradient_steps: 32
  learning_starts: 10
  policy_kwargs: "dict(log_std_init=-0.0821970906267695, net_arch=[64, 64])"